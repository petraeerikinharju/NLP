{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d31e0f-7cda-430e-875c-54a2137e661c",
   "metadata": {},
   "source": [
    "**I was able to complete most of the assignment. Some parts I was unsure how to do them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0502862-fa3f-4d6f-b89d-13207c430122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import csv\n",
    "from fuzzywuzzy import fuzz\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.test.utils import common_texts\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44468ce9-b95e-4adc-91d0-82af57a30b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "def preProcess(sentence):\n",
    "    \"\"\"Tokenize, remove stopwords, and clean the sentence.\"\"\"\n",
    "    Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word.lower() for word in words if word.isalpha() and word not in Stopwords] \n",
    "    return words\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character for lemmatization with WordNet.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n",
    "    return tag_dict.get(tag, wn.NOUN)  \n",
    "\n",
    "def wup(S1, S2):\n",
    "    \"\"\"Wu-Palmer similarity.\"\"\"\n",
    "    return S1.wup_similarity(S2)\n",
    "\n",
    "def resnik(S1, S2):\n",
    "    \"\"\"Resnik similarity.\"\"\"\n",
    "    return S1.res_similarity(S2, genesis_ic)\n",
    "\n",
    "options = {0: wup, 1: resnik}\n",
    "\n",
    "def word_similarity(w1, w2, num):\n",
    "    \"\"\"Calculate similarity between two words only if they share the same POS.\"\"\"\n",
    "    pos1 = get_wordnet_pos(w1)\n",
    "    pos2 = get_wordnet_pos(w2)\n",
    "\n",
    "    synsets1 = wn.synsets(w1, pos=pos1)\n",
    "    synsets2 = wn.synsets(w2, pos=pos2)\n",
    "    \n",
    "    if synsets1 and synsets2:\n",
    "        S1 = synsets1[0]  \n",
    "        S2 = synsets2[0]  \n",
    "        try:\n",
    "            similarity = options[num](S1, S2)\n",
    "            if similarity:\n",
    "                return round(similarity, 2)\n",
    "        except nltk.corpus.reader.wordnet.WordNetError:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "def Similarity(T1, T2, num):\n",
    "    \"\"\"Calculate sentence-to-sentence similarity using TF-IDF and WordNet similarity.\"\"\"\n",
    "    words1 = preProcess(T1)\n",
    "    words2 = preProcess(T2)\n",
    "\n",
    "    tf = TfidfVectorizer(use_idf=True)\n",
    "    tf.fit_transform([' '.join(words1), ' '.join(words2)])\n",
    "    \n",
    "    Idf = dict(zip(tf.get_feature_names_out(), tf.idf_))\n",
    "    \n",
    "    Sim_score1 = 0\n",
    "    Sim_score2 = 0\n",
    "\n",
    "    for w1 in words1:\n",
    "        Max = 0\n",
    "        for w2 in words2:\n",
    "            score = word_similarity(w1, w2, num)\n",
    "            if Max < score:\n",
    "                Max = score\n",
    "        Sim_score1 += Max * Idf.get(w1, 0)\n",
    "    Sim_score1 /= sum([Idf.get(w1, 0) for w1 in words1])\n",
    "\n",
    "    for w2 in words2:\n",
    "        Max = 0\n",
    "        for w1 in words1:\n",
    "            score = word_similarity(w1, w2, num)\n",
    "            if Max < score:\n",
    "                Max = score\n",
    "        Sim_score2 += Max * Idf.get(w2, 0)\n",
    "    Sim_score2 /= sum([Idf.get(w2, 0) for w2 in words2])\n",
    "\n",
    "    Sim = (Sim_score1 + Sim_score2) / 2\n",
    "    \n",
    "    return round(Sim, 2)\n",
    "\n",
    "model_w2v = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model_ft = FastText(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def average_embedding(model, tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            embeddings.append(model.wv[token])\n",
    "\n",
    "    if embeddings:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        return avg_embedding\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]\n",
    "\n",
    "def fuzzywuzzy_similarity(sentence1, sentence2):\n",
    "    return fuzz.ratio(sentence1, sentence2) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3193a9-0749-432e-b3b5-0d75cab23038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wup-sim Pearson correlation: 0.27, p-value: 0.00000\n",
      "Resnik-sim Pearson correlation: 0.09, p-value: 0.00000\n",
      "Word2Vec Pearson correlation: 0.04, p-value: 0.01003\n",
      "FastText Pearson correlation: 0.22, p-value: 0.00000\n",
      "FuzzyWuzzy Pearson correlation: 0.37, p-value: 0.00000\n"
     ]
    }
   ],
   "source": [
    "file_path = 'msr_paraphrase_corpus.txt'\n",
    "\n",
    "labels = []\n",
    "similarity_scores_wup = []\n",
    "similarity_scores_resnik = []\n",
    "similarity_scores_w2v = []\n",
    "similarity_scores_fasttext = []\n",
    "similarity_scores_fuzzywuzzy = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) >= 5:\n",
    "            label = int(row[0])\n",
    "            sentence1 = row[3]\n",
    "            sentence2 = row[4]\n",
    "\n",
    "            labels.append(label)\n",
    "            \"\"\"Wu-Palmer and Resnik\"\"\"\n",
    "            sim_score_wup = Similarity(sentence1, sentence2, num = 0)\n",
    "            similarity_scores_wup.append(sim_score_wup)\n",
    "            sim_score_resnik = Similarity(sentence1, sentence2, num = 1)\n",
    "            similarity_scores_resnik.append(sim_score_resnik)\n",
    "\n",
    "            \"\"\"Word2Vec\"\"\"\n",
    "            sentence1_tokens = word_tokenize(sentence1)\n",
    "            sentence2_tokens = word_tokenize(sentence2)\n",
    "            w2c_vector_1 = average_embedding(model_w2v, sentence1_tokens)\n",
    "            w2c_vector_2 = average_embedding(model_w2v, sentence2_tokens)\n",
    "            sim_score_w2v = cosine_sim(w2c_vector_1, w2c_vector_2)\n",
    "            similarity_scores_w2v.append(sim_score_w2v)\n",
    "\n",
    "\n",
    "            \"\"\"FastText\"\"\"\n",
    "            ft_vector_1 = average_embedding(model_ft, sentence1_tokens)\n",
    "            ft_vector_2 = average_embedding(model_ft, sentence2_tokens)\n",
    "            sim_score_ft = cosine_sim(ft_vector_1, ft_vector_2)\n",
    "            similarity_scores_fasttext.append(sim_score_ft)\n",
    "\n",
    "            \"\"\"FuzzyWuzzy\"\"\"\n",
    "            sim_score_fuzzywuzzy = fuzzywuzzy_similarity(sentence1, sentence2)\n",
    "            similarity_scores_fuzzywuzzy.append(sim_score_fuzzywuzzy)\n",
    "\n",
    "correlation_wup, p_value_wup = pearsonr(labels, similarity_scores_wup)\n",
    "correlation_resnik, p_value_resnik = pearsonr(labels, similarity_scores_resnik)\n",
    "correlation_w2v, p_value_w2v = pearsonr(labels, similarity_scores_w2v)\n",
    "correlation_ft, p_value_ft = pearsonr(labels, similarity_scores_fasttext)\n",
    "correlation_fuzzywuzzy, p_value_fuzzywuzzy = pearsonr(labels, similarity_scores_fuzzywuzzy)\n",
    "\n",
    "print(f\"Wup-sim Pearson correlation: {correlation_wup:.2f}, p-value: {p_value_wup:.5f}\")\n",
    "print(f\"Resnik-sim Pearson correlation: {correlation_resnik:.2f}, p-value: {p_value_resnik:.5f}\")\n",
    "\n",
    "print(f\"Word2Vec Pearson correlation: {correlation_w2v:.2f}, p-value: {p_value_w2v:.5f}\")\n",
    "print(f\"FastText Pearson correlation: {correlation_ft:.2f}, p-value: {p_value_ft:.5f}\")\n",
    "\n",
    "print(f\"FuzzyWuzzy Pearson correlation: {correlation_fuzzywuzzy:.2f}, p-value: {p_value_fuzzywuzzy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021f1f1-3ae3-4c9d-8224-b1fa9afde496",
   "metadata": {},
   "source": [
    "**None of the methods provide good results based on the Pearson correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0365353a-c223-4af6-81d2-9cab04ba85db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 447  832]\n",
      " [ 468 2194]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "similarity_scores = np.array(similarity_scores_wup)\n",
    "\n",
    "paraphrase_scores = similarity_scores[labels == 1]\n",
    "non_paraphrase_scores = similarity_scores[labels == 0]\n",
    "\n",
    "min_paraphrase = np.min(paraphrase_scores)\n",
    "max_non_paraphrase = np.max(non_paraphrase_scores)\n",
    "\n",
    "#Let's assign a threshold\n",
    "threshold = (min_paraphrase + max_non_paraphrase) / 2\n",
    "predictions = (similarity_scores >= threshold).astype(int)\n",
    "conf_matrix = confusion_matrix(labels, predictions)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef9737-42ac-4ef8-9487-b3224d27e8a6",
   "metadata": {},
   "source": [
    "**True negative: 447; True positive: 2194, False negative 468, False positive 832**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
