{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cc5bfe-769c-49ec-919a-c99b68ec627b",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c40559e-3a5a-4f1f-8bc3-33e036c250d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown, wordnet as wn, wordnet_ic, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import FastText, Word2Vec, Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01daba2f-b879-4cf9-9692-3efbaad89a2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee20bb0-29f2-4126-a551-f0c08c33d996",
   "metadata": {},
   "source": [
    "**Let's find synsets, hyponyms, hypernyms and various semantic similiarities for words \"rowan\" and \"crow\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67b901f-dc21-4f2f-9dfd-7adcccaf4383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('rowan.n.01')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('crow.n.01'),\n",
       " Synset('crow.n.02'),\n",
       " Synset('crow.n.03'),\n",
       " Synset('corvus.n.01'),\n",
       " Synset('brag.n.01'),\n",
       " Synset('crow.n.06'),\n",
       " Synset('gloat.v.01'),\n",
       " Synset('crow.v.02'),\n",
       " Synset('crow.v.03')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wn.synsets('rowan'))\n",
    "wn.synsets('crow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc3f97-3240-4d21-b05d-0ade3bd3c768",
   "metadata": {},
   "source": [
    "**For rowan there is only 1 synset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08994abd-114f-46f7-aa15-2cc6460a1d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eurasian tree with orange-red berrylike fruits'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('rowan.n.01').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b7f0d-d3ab-43e8-a05f-46cdc8e0c021",
   "metadata": {},
   "source": [
    "**For crow there is a lot more to cover. We will use the first one (crow.n.01) for this exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a08a05c-00ee-4f3b-91ca-88707771c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crow.n.01: black birds having a raucous call\n",
      "crow.n.02: the cry of a cock (or an imitation of it)\n",
      "crow.n.03: a member of the Siouan people formerly living in eastern Montana\n",
      "corvus.n.01: a small quadrilateral constellation in the southern hemisphere near Virgo\n",
      "brag.n.01: an instance of boastful talk\n",
      "crow.n.06: a Siouan language spoken by the Crow\n",
      "gloat.v.01: dwell on with satisfaction\n",
      "crow.v.02: express pleasure verbally\n",
      "crow.v.03: utter shrill sounds\n"
     ]
    }
   ],
   "source": [
    "crow_synsets = wn.synsets('crow')\n",
    "for syn in crow_synsets:\n",
    "    print(f\"{syn.name()}: {syn.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7064dd3a-ece1-48a8-a42b-dd1784abbfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('american_crow.n.01')]\n",
      "[Synset('corvine_bird.n.01')]\n"
     ]
    }
   ],
   "source": [
    "crow_bird = wn.synset('crow.n.01')\n",
    "types_of_crow = crow_bird.hyponyms()\n",
    "print(types_of_crow)\n",
    "crow_is_a = crow_bird.hypernyms()\n",
    "print(crow_is_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c8e7d2-9833-43a7-b321-b4a1de8e834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('organism.n.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowan = wn.synset('rowan.n.01')\n",
    "rowan.lowest_common_hypernyms(crow_bird)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef5c43-6626-4543-8fba-3cb8d6b7d4b5",
   "metadata": {},
   "source": [
    "**Then the script for retrieving the first hypernym and all hyponyms for 'car' and 'bus'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e2f6e8-b7e3-4f2d-a289-042c8b5ce4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hyper_hyponyms(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return None, None\n",
    "    first_synset = synsets[0]\n",
    "    hypernyms = first_synset.hypernyms()\n",
    "    first_hypernym = hypernyms[0] if hypernyms else None\n",
    "\n",
    "    hyponyms = first_synset.hyponyms()\n",
    "\n",
    "    return first_hypernym, hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575b7533-1803-45b4-8d28-3df0bc579e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first hypernym of car: Synset('motor_vehicle.n.01')\n",
      "hyponyms for cars:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('touring_car.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('used-car.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('ambulance.n.01'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('subcompact.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernym_car, hyponym_car = retrieve_hyper_hyponyms('car')\n",
    "print('first hypernym of car:', hypernym_car)\n",
    "print('hyponyms for cars:')\n",
    "hyponym_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bf09c2-7d22-4beb-bb25-7bf204632480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first hypernym of bus: Synset('public_transport.n.01')\n",
      "hyponyms for bus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('minibus.n.01'), Synset('trolleybus.n.01'), Synset('school_bus.n.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernym_bus, hyponym_bus = retrieve_hyper_hyponyms('bus')\n",
    "print('first hypernym of bus:', hypernym_bus)\n",
    "print('hyponyms for bus:')\n",
    "hyponym_bus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55891b9f-1077-4710-9821-5bdf026b3ae9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaea32e-ef96-4412-9be6-90ed15349bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Synset('car.n.01'), 71),\n",
       " (Synset('car.n.02'), 2),\n",
       " (Synset('car.n.03'), 0),\n",
       " (Synset('car.n.04'), 0),\n",
       " (Synset('cable_car.n.01'), 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_synset(word):\n",
    "    synsets = wn.synsets(word, 'n')\n",
    "    synset_count = [(synset, synset.lemmas()[0].count()) for synset in synsets]\n",
    "    synset_rank = sorted(synset_count, key = lambda x: x[1], reverse = True)\n",
    "    return synset_rank\n",
    "\n",
    "rank_synset('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3ceda-7365-4b11-823e-f13424e34fbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25429a0-e526-49ab-af17-108e26f82761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_average(list):\n",
    "    max_list = max(list)\n",
    "    min_list = min(list)\n",
    "    average = sum(list) / len(list)\n",
    "    return max_list, min_list, average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4711ad-7b2b-4be4-8742-03e5b6a54c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.96, 0.09523809523809523, 0.46739299830604175)\n"
     ]
    }
   ],
   "source": [
    "def wu_palmer_sim_synsets(word1, word2):\n",
    "    synsets_word1 = wn.synsets(word1, 'n')\n",
    "    synsets_word2 = wn.synsets(word2, 'n')\n",
    "    \n",
    "    similarity_score = []\n",
    "    \n",
    "    for syn1 in synsets_word1:\n",
    "        for syn2 in synsets_word2:\n",
    "            similarity = syn1.wup_similarity(syn2)\n",
    "            similarity_score.append(similarity)\n",
    "\n",
    "    return max_min_average(similarity_score)\n",
    "\n",
    "print(wu_palmer_sim_synsets('car', 'bus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ae4096-1f3e-4008-bde5-012319d11522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('motor_vehicle.n.01') Synset('public_transport.n.01')\n",
      "0.7368421052631579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7368421052631579, 0.7368421052631579, 0.7368421052631579)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_hypernym_car = wn.synsets('car', 'n')[0].hypernyms()[0]\n",
    "first_hypernym_bus = wn.synsets('bus', 'n')[0].hypernyms()[0]\n",
    "print(first_hypernym_car, first_hypernym_bus)\n",
    "print(first_hypernym_car.wup_similarity(first_hypernym_bus))\n",
    "\n",
    "wu_palmer_sim_synsets(first_hypernym_car.name().split('.')[0], first_hypernym_bus.name().split('.')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "700ce2ef-a21f-4b35-9378-dcd49f2b025c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6086956521739131, 0.6086956521739131, 0.6086956521739131)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hyponym_wu_palmer_sim_synsets(word1, word2):\n",
    "    hyponyms_word1 = wn.synsets(word1, 'n')[0].hyponyms()\n",
    "    hyponyms_word2 = wn.synsets(word2, 'n')[0].hyponyms()\n",
    "\n",
    "    similarity_score = []\n",
    "\n",
    "    for hyponym1 in hyponyms_word1:\n",
    "        for hyponym2 in hyponyms_word2:\n",
    "            similarity = hyponym1.wup_similarity(hyponym2)\n",
    "            similarity_score.append(similarity)\n",
    "\n",
    "    return max_min_average(similarity_score)\n",
    "\n",
    "hyponym_wu_palmer_sim_synsets('car', 'bus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabc05d-bb21-4535-91d3-41b4cfac9301",
   "metadata": {},
   "source": [
    "**Because we use wup-similarity and all the hyponyms are as far away from each other as every other one, the similarity is the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8867d-2f86-4dbd-996a-82789f602a30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "076b5212-0b4d-4e6e-99bb-d2dc2c4492fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.34659468740185323, 0.05161364962677664, 0.09387159388812354)\n"
     ]
    }
   ],
   "source": [
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "def jcn_sim_synsets(word1, word2):\n",
    "    synsets_word1 = wn.synsets(word1, 'n')\n",
    "    synsets_word2 = wn.synsets(word2, 'n')\n",
    "    \n",
    "    similarity_score = []\n",
    "    \n",
    "    for syn1 in synsets_word1:\n",
    "        for syn2 in synsets_word2:\n",
    "            similarity = syn1.jcn_similarity(syn2, brown_ic)\n",
    "            similarity_score.append(similarity)\n",
    "\n",
    "    return max_min_average(similarity_score)\n",
    "\n",
    "print(jcn_sim_synsets('car', 'bus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14535a83-f703-4845-8df0-0ee2de90f8db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70837d7a-486e-433d-abf3-82af007d1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes: 0.28281489889469863\n",
      "Lowercase and stopword removal: 0.30624163374802627\n",
      "Lowercase, stopword removal and lemmatization: 0.26785709895261356\n"
     ]
    }
   ],
   "source": [
    "def idf_calc(word, docs):\n",
    "    word_amount = sum(1 for doc in docs if word in doc)\n",
    "    if word_amount > 0:\n",
    "        return np.log(len(docs) / word_amount)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def max_similarity(word, tokens):\n",
    "    max_sim = 0\n",
    "    for token in tokens:\n",
    "        syn1 = wn.synsets(word)\n",
    "        syn2 = wn.synsets(token)\n",
    "        if syn1 and syn2:\n",
    "            sim = syn1[0].wup_similarity(syn2[0])\n",
    "            if sim and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "def mihalcea_similarity(text1_tokens, text2_tokens, tokenized=True):\n",
    "    if not tokenized:\n",
    "        text1_tokens = word_tokenize(text1_tokens)\n",
    "        text2_tokens = word_tokenize(text2_tokens)\n",
    "    \n",
    "    unique_words = set(text1_tokens + text2_tokens)\n",
    "    idf_values = {word: idf_calc(word, [text1_tokens, text2_tokens]) for word in unique_words}\n",
    "    max_text1 = {word: max_similarity(word, text2_tokens) for word in unique_words}\n",
    "    max_text2 = {word: max_similarity(word, text1_tokens) for word in unique_words}\n",
    "\n",
    "    sum_max_text1 = sum(max_text1[word] * idf_values[word] for word in unique_words)\n",
    "    sum_max_text2 = sum(max_text2[word] * idf_values[word] for word in unique_words)\n",
    "    idf_sum = sum(idf_values[word] for word in unique_words)\n",
    "\n",
    "    similarity_score = float(1/2 * ((sum_max_text1 / idf_sum) * (sum_max_text2 / idf_sum)))\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "T1 = \"Students feel unhappy today about the class today\"\n",
    "T2 = \"Several students study hard at classes in recent days\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "T1_tokens = word_tokenize(T1.lower())\n",
    "T1_stop = [word for word in T1_tokens if word not in stop_words]\n",
    "T2_tokens = word_tokenize(T2.lower())\n",
    "T2_stop = [word for word in T2_tokens if word not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "T1_clean = [lemmatizer.lemmatize(word) for word in T1_stop]\n",
    "T2_clean = [lemmatizer.lemmatize(word) for word in T2_stop]\n",
    "\n",
    "print(\"No changes:\", mihalcea_similarity(T1, T2, False))\n",
    "print(\"Lowercase and stopword removal:\", mihalcea_similarity(T1_stop, T2_stop))\n",
    "print(\"Lowercase, stopword removal and lemmatization:\", mihalcea_similarity(T1_clean, T2_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21e105-1ee8-4d9e-9a92-1f161fce23a1",
   "metadata": {},
   "source": [
    "**When using lemmatization (or stemming) the similarity drops because of how the similarity is calculated in the wup_similarity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ab396-5c2f-4603-82aa-b05fbb57118d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee53ad6-e794-46bc-8162-481ecc65d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun-transformation: 0.2645075291145569\n"
     ]
    }
   ],
   "source": [
    "def noun_transformation(tokens):\n",
    "    noun_tokens = []\n",
    "    for token in tokens:\n",
    "        noun = wn.morphy(token, wn.NOUN)\n",
    "        if noun:\n",
    "            noun_tokens.append(noun)\n",
    "        else:\n",
    "            noun_tokens.append(token)\n",
    "    return noun_tokens\n",
    "\n",
    "T1_noun = noun_transformation(T1_stop)\n",
    "T2_noun = noun_transformation(T2_stop)\n",
    "\n",
    "print(\"Noun-transformation:\", mihalcea_similarity(T1_noun, T2_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8532014-f86d-4531-94cd-dcae5ef8c7d3",
   "metadata": {},
   "source": [
    "**We get a lower similarity because of the noun-transformation. Again, we might get a better similarity if we'd use a different similarity than wup_similarity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96332c9b-b37a-4eb0-81d9-225bf9c7d730",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa131e9-8580-4eb5-b6c0-6bec8b5bcae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext: 0.05813166\n",
      "word2vec: 0.07250185\n"
     ]
    }
   ],
   "source": [
    "data = [T1_tokens, T2_tokens]\n",
    "\n",
    "fasttext_model = FastText(sentences=data, vector_size=300, window=5, min_count=1)\n",
    "word2vec_model = Word2Vec(sentences=data, vector_size=300, window=5, min_count=1)\n",
    "\n",
    "def average_embedding(model, tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            embeddings.append(model.wv[token])\n",
    "\n",
    "    if embeddings:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        return avg_embedding\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]\n",
    "\n",
    "fasttext_vector_T1 = average_embedding(fasttext_model, T1_tokens)\n",
    "fasttext_vector_T2 = average_embedding(fasttext_model, T2_tokens)\n",
    "fasttext_similarity = cosine_sim(fasttext_vector_T1, fasttext_vector_T2)\n",
    "\n",
    "word2vec_vector_T1 = average_embedding(word2vec_model, T1_tokens)\n",
    "word2vec_vector_T2 = average_embedding(word2vec_model, T2_tokens)\n",
    "word2vec_similarity = cosine_sim(word2vec_vector_T1, word2vec_vector_T2)\n",
    "\n",
    "print(\"Fasttext:\", fasttext_similarity)\n",
    "print(\"word2vec:\", word2vec_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef8092-2b1a-48ab-9b1e-7f889df93956",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c7fe4-49b9-47b2-9fb8-ae017cebcc69",
   "metadata": {},
   "source": [
    "**We already have the cleaned tokens in T1_clean and T2_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28906a56-0b20-40da-b8da-d32947805d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "fuzzy_similarity = fuzz.ratio(T1_clean, T2_clean)\n",
    "print(fuzzy_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e1a8b-efd5-45de-9433-7b39ab350d94",
   "metadata": {},
   "source": [
    "### Final words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79fc75-e281-495d-84f9-40390464273f",
   "metadata": {},
   "source": [
    "This was an interesting assignment. I had much better time to prepare for this, so I actually completed this this time. Although in the Task 7 I had trouble getting the doc2vec working, everything else went fairly smoothly. Took me around 4 hours to complete this whole set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0fe31-ce99-45fc-909d-a4d2d2a5e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
