{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec06da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5783551-d5ec-4351-8df4-ac4756f4379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Module doesn't exist\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from SOC_PMI.main import similarity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "stopwords = list(set(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053f867",
   "metadata": {},
   "source": [
    "### Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b82757-638d-48f0-bcf4-cb3c9c8072fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List lengths: 66, 66, 66\n",
      "Pearson correlation coefficient Sim1: 0.67\n"
     ]
    }
   ],
   "source": [
    "def preProcess(sentence):\n",
    "    \"\"\"Preprocess a single sentence by tokenizing, converting to lowercase, and removing stop words.\"\"\"\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence.lower())\n",
    "    filtered_sentence = [word for word in tokenized_sentence if word not in stopwords]\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def sim1(sentence_list):\n",
    "    \"\"\"Calculate sentence-to-sentence similarity using TF-IDF and WordNet similarity.\"\"\"\n",
    "    computed_similarities = []\n",
    "    tf = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "    for T1, T2 in sentence_list:\n",
    "        words1 = preProcess(T1)\n",
    "        words2 = preProcess(T2)\n",
    "\n",
    "        tf_matrix = tf.fit_transform([' '.join(words1), ' '.join(words2)])\n",
    "        \n",
    "        sim_score = cosine_similarity(tf_matrix[0:1], tf_matrix[1:2])[0][0]\n",
    "        computed_similarities.append(round(sim_score, 2))\n",
    "\n",
    "    return computed_similarities\n",
    "\n",
    "def read_from_csv(file_path):\n",
    "    \"\"\"Read sentences and the corresponding similarity scores from a csv file using pandas.\"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path, delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Ensure the DataFrame has the correct number of columns\n",
    "    if df.shape[1] != 3:\n",
    "        raise ValueError(\"CSV file must contain exactly 3 columns.\")\n",
    "\n",
    "    # Extract sentences and scores\n",
    "    sentences = list(zip(df.iloc[:, 0].str.strip(), df.iloc[:, 1].str.strip()))\n",
    "    scores = df.iloc[:, 2].astype(float).tolist()\n",
    "    \n",
    "    return sentences, scores\n",
    "\n",
    "sentences_stss, human_similarities = read_from_csv(\"STSS-131.csv\")\n",
    "\n",
    "computed_similarities_1 = sim1(sentences_stss)\n",
    "\n",
    "print(f\"List lengths: {len(sentences_stss)}, {len(human_similarities)}, {len(computed_similarities_1)}\")\n",
    "\n",
    "pearson_coeff_1, _ = pearsonr(human_similarities, computed_similarities_1)\n",
    "\n",
    "print(f\"Pearson correlation coefficient Sim1: {pearson_coeff_1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e585400",
   "metadata": {},
   "source": [
    "### Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e561d7-e09e-4279-9638-6fe04739b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between:\n",
      "'The city was noisy.'\n",
      "and\n",
      "'The forest was silent.'\n",
      "is: 0.3333\n",
      "\n",
      "Similarity between:\n",
      "'Did you finish your homework?'\n",
      "and\n",
      "'Have you completed your assignments?'\n",
      "is: 0.4125\n",
      "\n",
      "Similarity between:\n",
      "'The cat sat on the warm windowsill.'\n",
      "and\n",
      "'A cat rested on a cozy window ledge.'\n",
      "is: 0.3089\n",
      "\n",
      "Similarity between:\n",
      "'He does not like apples.'\n",
      "and\n",
      "'He dislike apples.'\n",
      "is: 0.5212\n",
      "\n",
      "Similarity between:\n",
      "'The food was delicious.'\n",
      "and\n",
      "'The meal was tasty.'\n",
      "is: 0.5417\n",
      "\n",
      "Similarity between:\n",
      "'The quick brown fox jumps over the lazy dog.'\n",
      "and\n",
      "'A quick fox leaps over a lazy hound.'\n",
      "is: 0.3538\n",
      "\n",
      "Similarity between:\n",
      "'She is not happy with the results.'\n",
      "and\n",
      "'She is sad with the results.'\n",
      "is: 1.0000\n",
      "\n",
      "Similarity between:\n",
      "'Apple Inc. released a new product.'\n",
      "and\n",
      "'Google LLC announced their latest software.'\n",
      "is: 0.2584\n",
      "\n",
      "Similarity between:\n",
      "'He did not find the answer quickly.'\n",
      "and\n",
      "'He found the answer slowly.'\n",
      "is: 0.4510\n",
      "\n",
      "Similarity between:\n",
      "'NASA announced a new space mission.'\n",
      "and\n",
      "'The European Space Agency confirmed another mission.'\n",
      "is: 0.3413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negations = {\"not\", \"no\", \"never\"}\n",
    "\n",
    "synset_cache = {}\n",
    "\n",
    "def get_synsets(word):\n",
    "    \"\"\"Get synsets for a word using a cache to avoid recalculating.\"\"\"\n",
    "    if word not in synset_cache:\n",
    "        synset_cache[word] = wn.synsets(word)\n",
    "    return synset_cache[word]\n",
    "\n",
    "def antonym(word):\n",
    "    \"\"\"Get the antonym of a word using WordNet.\"\"\"\n",
    "    synonyms = wn.synsets(word)\n",
    "    for syn in synonyms:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return word\n",
    "\n",
    "def process_negation(tokens):\n",
    "    \"\"\"Process negation in tokens and convert adjectives/adverbs to their antonyms.\"\"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in negations:\n",
    "            negation_index = i\n",
    "            # Check for adjectives/adverbs after the negation\n",
    "            for j in range(negation_index + 1, len(tokens)):\n",
    "                if pos_tag([tokens[j]])[0][1].startswith('JJ') or pos_tag([tokens[j]])[0][1].startswith('RB'):\n",
    "                    tokens[j] = antonym(tokens[j])  # Change to antonym\n",
    "                    break\n",
    "    return tokens\n",
    "\n",
    "def preprocess_and_extract_nouns(sentence):\n",
    "    \"\"\"Preprocess sentence to extract noun entities and process negation.\"\"\"\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    tokens = process_negation(tokens)\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stopwords]\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    named_entities = [ent.text for ent in doc.ents]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    noun_tokens = []\n",
    "    \n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag.startswith('NN') or tag.startswith('VB'):\n",
    "            noun_tokens.append(token)\n",
    "\n",
    "    # Convert verbs, adjectives/adverbs to nouns using WordNet\n",
    "    noun_tokens = list(set(noun_tokens))\n",
    "    final_nouns = []\n",
    "    for token in noun_tokens:\n",
    "        synsets = wn.synsets(token)\n",
    "        if synsets:\n",
    "            final_nouns.append(synsets[0].lemmas()[0].name())\n",
    "\n",
    "    return final_nouns, named_entities\n",
    "\n",
    "def wu_palmer_similarity(nouns1, nouns2):\n",
    "    \"\"\"Calculate Wu-Palmer similarity for all noun pairs.\"\"\"\n",
    "    if not nouns1 or not nouns2:\n",
    "        return 0.0\n",
    "    similarity_scores = []\n",
    "    for n1 in nouns1:\n",
    "        for n2 in nouns2:\n",
    "            syn1 = wn.synsets(n1)\n",
    "            syn2 = wn.synsets(n2)\n",
    "            if syn1 and syn2:\n",
    "                sim_score = syn1[0].wup_similarity(syn2[0])\n",
    "                if sim_score is not None:\n",
    "                    similarity_scores.append(sim_score)\n",
    "    return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0\n",
    "\n",
    "def sim2(sentence_list):\n",
    "    \"\"\"Calculate similarity for a list of sentence pairs based on named entities and semantic similarity.\"\"\"\n",
    "    computed_similarities = []\n",
    "\n",
    "    for S1, S2 in sentence_list:\n",
    "        nouns1, named_entities1 = preprocess_and_extract_nouns(S1)\n",
    "        nouns2, named_entities2 = preprocess_and_extract_nouns(S2)\n",
    "\n",
    "        if named_entities1 and named_entities2:\n",
    "            # Both sentences contain named entities\n",
    "            entity_similarity = cosine_similarity(\n",
    "                [nlp(' '.join(named_entities1)).vector],\n",
    "                [nlp(' '.join(named_entities2)).vector]\n",
    "            )[0][0]\n",
    "\n",
    "            semantic_similarity = wu_palmer_similarity(nouns1, nouns2)\n",
    "            final_similarity = 0.5 * entity_similarity + 0.5 * semantic_similarity\n",
    "        elif not named_entities1 and not named_entities2:\n",
    "            # No named entities in either sentence\n",
    "            final_similarity = wu_palmer_similarity(nouns1, nouns2)\n",
    "        else:\n",
    "            # One sentence has named entities, discard named entities\n",
    "            final_similarity = wu_palmer_similarity(nouns1, nouns2)\n",
    "\n",
    "        computed_similarities.append(final_similarity)\n",
    "\n",
    "    return computed_similarities\n",
    "\n",
    "# Test with 10 sentence pairs\n",
    "test_pairs = [\n",
    "    (\"The city was noisy.\", \"The forest was silent.\"),\n",
    "    (\"Did you finish your homework?\", \"Have you completed your assignments?\"),\n",
    "    (\"The cat sat on the warm windowsill.\", \"A cat rested on a cozy window ledge.\"),\n",
    "    (\"He does not like apples.\", \"He dislike apples.\"),\n",
    "    (\"The food was delicious.\", \"The meal was tasty.\"),\n",
    "    (\"The quick brown fox jumps over the lazy dog.\", \"A quick fox leaps over a lazy hound.\"),\n",
    "    (\"She is not happy with the results.\", \"She is sad with the results.\"),\n",
    "    (\"Apple Inc. released a new product.\", \"Google LLC announced their latest software.\"),\n",
    "    (\"He did not find the answer quickly.\", \"He found the answer slowly.\"),\n",
    "    (\"NASA announced a new space mission.\", \"The European Space Agency confirmed another mission.\"),\n",
    "]\n",
    "\n",
    "computed_similarities = sim2(test_pairs)\n",
    "\n",
    "for (S1, S2), sim2_score in zip(test_pairs, computed_similarities):\n",
    "    print(f\"Similarity between:\\n'{S1}'\\nand\\n'{S2}'\\nis: {sim2_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034b25e",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e04ca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient Sim1: 0.67\n",
      "Pearson correlation coefficient Sim2: 0.32\n"
     ]
    }
   ],
   "source": [
    "computed_similarities_2 = sim2(sentences_stss)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Sentence 1': [s[0] for s in sentences_stss],\n",
    "    'Sentence 2': [s[1] for s in sentences_stss],\n",
    "    'Human Similarity': human_similarities,\n",
    "    'Computed Similarity Sim1': computed_similarities_1,\n",
    "    'Computed Similarity Sim2': computed_similarities_2\n",
    "})\n",
    "\n",
    "#'''You can see the table in the GitHub'''\n",
    "#df.to_excel('similarities.xlsx', index=False)\n",
    "\n",
    "pearson_coeff_2, _ = pearsonr(human_similarities, computed_similarities_2)\n",
    "\n",
    "print(f\"Pearson correlation coefficient Sim1: {pearson_coeff_1:.2f}\")\n",
    "print(f\"Pearson correlation coefficient Sim2: {pearson_coeff_2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731f753",
   "metadata": {},
   "source": [
    "The pearson correlation yields a worse answer. Meaning that the method of turning everything to nouns, the antonym preprocessing and using the named-entities preprocessing give us worse results than with just the preprocesses used in task 1. This may be due to the added complexity, which weren't accounted for in the human similarity judgments.\n",
    "The preprocessing methods may also lead to a loss of important semantic information. Named entities are supposed to help by focusing on specific terms, but if they are not well aligned with the context of the sentences, they may add more noise rather than clarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73060b3d",
   "metadata": {},
   "source": [
    "### Task 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5312556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient with Doc2Vec: 0.65\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_doc2vec(sentence_list, epochs=200):\n",
    "    \"\"\"Train a Doc2Vec model using a list of sentence pairs.\"\"\"\n",
    "\n",
    "    tagged_data = [TaggedDocument(words=preProcess(s[0]) + preProcess(s[1]), tags=[str(i)]) for i, s in enumerate(sentence_list)]\n",
    "\n",
    "    doc2vec_model = Doc2Vec(vector_size=200, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1, epochs=epochs)\n",
    "    doc2vec_model.build_vocab(tagged_data)\n",
    "    doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "    computed_similarities_doc2vec = []\n",
    "    for sentence1, sentence2 in sentence_list:\n",
    "        try:\n",
    "            vec1 = doc2vec_model.infer_vector(preProcess(sentence1))\n",
    "            vec2 = doc2vec_model.infer_vector(preProcess(sentence2))\n",
    "            similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            computed_similarities_doc2vec.append(similarity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair ({sentence1}, {sentence2}): {e}\")\n",
    "\n",
    "    return computed_similarities_doc2vec, doc2vec_model\n",
    "\n",
    "computed_similarities_doc2vec, model = compute_similarity_doc2vec(sentences_stss)\n",
    "\n",
    "pearson_coeff_doc2vec = pearsonr(human_similarities, computed_similarities_doc2vec)[0]\n",
    "print(f\"Pearson correlation coefficient with Doc2Vec: {pearson_coeff_doc2vec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe51ad",
   "metadata": {},
   "source": [
    "By tuning the epochs, we got a better pearson correlation. With only 100 epochs, the coefficient was 0.12. But after 200 epochs the coefficient drops again, indicating overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dde0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient with SpaCy embedding: 0.54\n"
     ]
    }
   ],
   "source": [
    "def compute_spacy_embeddings(sentence_list):\n",
    "    \"\"\"Compute SpaCy embeddings for a list of sentence pairs.\"\"\"\n",
    "    computed_similarities = []\n",
    "    for sentence1, sentence2 in sentence_list:\n",
    "        # Generate embeddings using SpaCy\n",
    "        vec1 = nlp(sentence1).vector\n",
    "        vec2 = nlp(sentence2).vector\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "        computed_similarities.append(similarity)\n",
    "    return computed_similarities\n",
    "\n",
    "computed_similarities_spacy_e = compute_spacy_embeddings(sentences_stss)\n",
    "pearson_coeff_spacy_e = pearsonr(human_similarities, computed_similarities_spacy_e)[0]\n",
    "print(f\"Pearson correlation coefficient with SpaCy embedding: {pearson_coeff_spacy_e:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26599f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient with DistilBERT embedding: 0.84\n"
     ]
    }
   ],
   "source": [
    "def compute_distilbert_embeddings(sentence_list):\n",
    "    \"\"\"Compute DistilBERT embeddings for a list of sentence pairs.\"\"\"\n",
    "    computed_similarities = []\n",
    "    for sentence1, sentence2 in sentence_list:\n",
    "        inputs1 = tokenizer(sentence1, padding=True, truncation=True, return_tensors='pt')\n",
    "        inputs2 = tokenizer(sentence2, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs1 = distilbert_model(**inputs1)\n",
    "            outputs2 = distilbert_model(**inputs2)\n",
    "\n",
    "        vec1 = outputs1.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        vec2 = outputs2.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "        similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "        computed_similarities.append(similarity)\n",
    "    return computed_similarities\n",
    "\n",
    "computed_similarities_distilbert_e = compute_distilbert_embeddings(sentences_stss)\n",
    "pearson_coeff_distilbert_e = pearsonr(human_similarities, computed_similarities_distilbert_e)[0]\n",
    "print(f\"Pearson correlation coefficient with DistilBERT embedding: {pearson_coeff_distilbert_e:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26105268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_similarity_use(sentence_list):\n",
    "#    \"\"\"Compute cosine similarity for a list of sentence pairs using the Universal Sentence Encoder.\"\"\"\n",
    "#    computed_similarities = []\n",
    "#    \n",
    "#    for sentence1, sentence2 in sentence_list:\n",
    "#        embeddings = embed([sentence1, sentence2]).numpy()\n",
    "#        \n",
    "#        similarity = cosine_similarity(embeddings)[0, 1]\n",
    "#        computed_similarities.append(similarity)\n",
    "#\n",
    "#   return computed_similarities\n",
    "  \n",
    "\n",
    "#computed_similarities_use = compute_similarity_use(sentences_stss)\n",
    "#pearson_coeff_use, _ = pearsonr(human_similarities, computed_similarities_use)\n",
    "#print(f\"Pearson correlation coefficient with Universal Sentence Encoder: {pearson_coeff_use:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de97479",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27128de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 5500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 2600\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "})\n",
      "Extracted 200 sentence pairs from the train set.\n",
      "Extracted 200 sentence pairs from the test set.\n",
      "Extracted 200 sentence pairs from the dev set.\n",
      "Train Results:\n",
      "  sim1: 0.62\n",
      "  sim2: 0.37\n",
      "  doc2vec: 0.39\n",
      "  SpaCy: 0.41\n",
      "  DistilBERT: 0.59\n",
      "Test Results:\n",
      "  sim1: 0.69\n",
      "  sim2: 0.52\n",
      "  doc2vec: 0.53\n",
      "  SpaCy: 0.42\n",
      "  DistilBERT: 0.71\n",
      "Dev Results:\n",
      "  sim1: 0.68\n",
      "  sim2: 0.42\n",
      "  doc2vec: 0.45\n",
      "  SpaCy: 0.39\n",
      "  DistilBERT: 0.64\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"SemRel/SemRel2024\", \"eng\")\n",
    "\n",
    "print(ds)\n",
    "\n",
    "datasets = [\"train\", \"test\", \"dev\"]\n",
    "\n",
    "def extract_sentences_and_labels(dataset_name):\n",
    "    '''Extract sentences and labels from a dataset from SemRel2024'''\n",
    "    \n",
    "    dataset = ds[dataset_name].shuffle(seed=42)\n",
    "    dataset = dataset.select(range(200))\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for item in dataset:\n",
    "        sentence1 = item['sentence1'].strip()\n",
    "        sentence2 = item['sentence2'].strip()\n",
    "        label = float(item['label'])\n",
    "\n",
    "        sentences.append((sentence1, sentence2))\n",
    "        labels.append(label)\n",
    "        \n",
    "    print(f\"Extracted {len(sentences)} sentence pairs from the {dataset_name} set.\")\n",
    "    return sentences, labels \n",
    "\n",
    "results = {}\n",
    "stored_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    '''Use all the previous methods for SemRel2024 datasets'''\n",
    "\n",
    "    sentences, labels = extract_sentences_and_labels(dataset)\n",
    "\n",
    "    sim1_scores = sim1(sentences)\n",
    "    sim2_scores = sim2(sentences)\n",
    "    doc2vec_scores, model = compute_similarity_doc2vec(sentences, 150)\n",
    "    spacy_scores = compute_spacy_embeddings(sentences)\n",
    "    distilbert_scores = compute_distilbert_embeddings(sentences)\n",
    "    #use_scores = compute_similarity_use(sentences)\n",
    "\n",
    "    stored_scores[dataset] = {\n",
    "        'doc2vec': doc2vec_scores,\n",
    "        'spacy': spacy_scores,\n",
    "        'distilbert': distilbert_scores\n",
    "    }\n",
    "\n",
    "    # Calculate Pearson correlation coefficients\n",
    "    sim1_corr = pearsonr(sim1_scores, labels)[0]\n",
    "    sim2_corr = pearsonr(sim2_scores, labels)[0]\n",
    "    doc2vec_corr = pearsonr(doc2vec_scores, labels)[0]\n",
    "    spacy_corr = pearsonr(spacy_scores, labels)[0]\n",
    "    distilbert_corr = pearsonr(distilbert_scores, labels)[0]\n",
    "    #use_corr = pearsonr(use_scores, labels)[0]\n",
    "        \n",
    "    results[dataset] = {\n",
    "        'sim1': sim1_corr,\n",
    "        'sim2': sim2_corr,\n",
    "        'doc2vec': doc2vec_corr,\n",
    "        'SpaCy': spacy_corr,\n",
    "        'DistilBERT': distilbert_corr\n",
    "        #'use': use_corr,\n",
    "    }\n",
    "    \n",
    "for dataset, correlations in results.items():\n",
    "    print(f\"{dataset.capitalize()} Results:\")\n",
    "    for method, corr in correlations.items():\n",
    "        print(f\"  {method}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716692f",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939b822",
   "metadata": {},
   "source": [
    "We need to weight the different models and see which weights produce the biggest coefficient. The Distilbert seemed to give the best answer constistently so let's weight that more. SpaCy and Doc2Vec performed around as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493eda23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 200 sentence pairs from the train set.\n",
      "Extracted 200 sentence pairs from the test set.\n",
      "Extracted 200 sentence pairs from the dev set.\n",
      "Train Results:\n",
      "  Ensemble: 0.63\n",
      "Test Results:\n",
      "  Ensemble: 0.73\n",
      "Dev Results:\n",
      "  Ensemble: 0.68\n"
     ]
    }
   ],
   "source": [
    "weights = [0.25, 0.25, 0.5]\n",
    "\n",
    "for dataset in datasets:\n",
    "    '''Use all the previous methods for SemRel2024 datasets'''\n",
    "    sentences, labels = extract_sentences_and_labels(dataset)\n",
    "    \n",
    "    doc2vec_scores = stored_scores[dataset]['doc2vec']\n",
    "    spacy_scores = stored_scores[dataset]['spacy']\n",
    "    distilbert_scores = stored_scores[dataset]['distilbert']\n",
    "    \n",
    "    ensemble_scores = [\n",
    "        sum(w * sim for w, sim in zip(weights, similarities))\n",
    "        for similarities in zip(doc2vec_scores, spacy_scores, distilbert_scores)\n",
    "    ]\n",
    "    ensemble_corr = pearsonr(ensemble_scores, labels)[0]\n",
    "    results[dataset] = {\n",
    "        'Ensemble': ensemble_corr\n",
    "    }\n",
    "    \n",
    "for dataset, correlations in results.items():\n",
    "    print(f\"{dataset.capitalize()} Results:\")\n",
    "    for method, corr in correlations.items():\n",
    "        print(f\"  {method}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870de8d0",
   "metadata": {},
   "source": [
    "This performs (slightly) better than any other method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec46a7",
   "metadata": {},
   "source": [
    "### Task 7 & 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient SOC-PMI-Short-Text-Similarity-: 0.67\n"
     ]
    }
   ],
   "source": [
    "SOC_similarities = []\n",
    "for S1, S2 in sentences_stss:\n",
    "    sim_score = similarity(S1, S2) # Call the similarity from the provided repository\n",
    "    SOC_similarities.append(sim_score)\n",
    "    \n",
    "SOC_coefficient = pearsonr(SOC_similarities, human_similarities)[0]\n",
    "print(f\"Pearson correlation coefficient SOC-PMI-Short-Text-Similarity-: {pearson_coeff_1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9948e",
   "metadata": {},
   "source": [
    "### Task 9 (Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a69eb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Sentence Similarity Checker!\n",
      "\n",
      "Please enter a pair of sentences:\n",
      "You input sentences: \" The cat lounged on the windowsill, basking in the warm sunlight. \" and \" The dog rested on the porch, enjoying the cool evening breeze. \"\n",
      "Similarity Scores:\n",
      "Sim1 (TF-IDF + WordNet): 0.0000\n",
      "Sim2 (Wu-Palmer + Named Entities): 0.2712\n",
      "Doc2Vec Similarity: 0.9702\n",
      "SpaCy Similarity: 0.8733\n",
      "DistilBERT Similarity: 0.9402\n",
      "\n",
      "Please enter a pair of sentences:\n",
      "You input sentences: \" The cat curled up on the couch and fell asleep. \" and \" The cat snuggled on the couch and drifted off to sleep. \"\n",
      "Similarity Scores:\n",
      "Sim1 (TF-IDF + WordNet): 0.2500\n",
      "Sim2 (Wu-Palmer + Named Entities): 0.2492\n",
      "Doc2Vec Similarity: 0.8576\n",
      "SpaCy Similarity: 0.9757\n",
      "DistilBERT Similarity: 0.9659\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Welcome to the Sentence Similarity Checker!\")\n",
    "    while True:\n",
    "        print(\"\\nPlease enter a pair of sentences:\")\n",
    "        sentence1 = input(\"Sentence 1: \")\n",
    "        sentence2 = input(\"Sentence 2: \")\n",
    "        \n",
    "        # Call similarity functions\n",
    "        sentences_pair = [(sentence1, sentence2)]\n",
    "        computed_similarities_1 = sim1(sentences_pair)\n",
    "        computed_similarities_2 = sim2(sentences_pair)\n",
    "        computed_similarities_doc2vec, _ = compute_similarity_doc2vec(sentences_pair)\n",
    "        computed_similarities_spacy = compute_spacy_embeddings(sentences_pair)\n",
    "        computed_similarities_distilbert = compute_distilbert_embeddings(sentences_pair)\n",
    "\n",
    "        # Display results\n",
    "        print(\"You input sentences: \\\"\", sentence1, \"\\\" and \\\"\", sentence2, \"\\\"\")\n",
    "        print(\"Similarity Scores:\")\n",
    "        print(f\"Sim1 (TF-IDF + WordNet): {computed_similarities_1[0]:.4f}\")\n",
    "        print(f\"Sim2 (Wu-Palmer + Named Entities): {computed_similarities_2[0]:.4f}\")\n",
    "        print(f\"Doc2Vec Similarity: {computed_similarities_doc2vec[0]:.4f}\")\n",
    "        print(f\"SpaCy Similarity: {computed_similarities_spacy[0]:.4f}\")\n",
    "        print(f\"DistilBERT Similarity: {computed_similarities_distilbert[0]:.4f}\")\n",
    "        \n",
    "        cont = input(\"\\nDo you want to check another pair? (y/n): \")\n",
    "        if cont.lower() != 'y':\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
